---
theme: night
---

## Web Audio Introduction

##### HS23/24 - HKB - Laurens Inauen

---

### Course Materials

course materials are hosted on [Github](https://github.com/laurens-in/WebAudioIntroduction) to update run `git pull`

---

| | |
|---|---|
|1st half|Samples: theory & peer programming|
|2nd half|AudioWorklet: theory & peer programming|

---

Exercises?

---

### Web Audio API

Working with samples

---

### Working with buffers

buffer playback with the Web Audio API is simple:

```js
// create a buffer source
const source = audioCtx.createBufferSource();
// set the buffer in the AudioBufferSourceNode
source.buffer = myArrayBuffer;
// connect the AudioBufferSourceNode to the
// destination so we can hear the sound
source.connect(audioCtx.destination);
// start the source playing
source.start();
```

---

### Working with buffers

We can fill a buffer with anything we want:

```js
// Create an empty three-second stereo buffer at the sample rate of the AudioContext
const myArrayBuffer = audioCtx.createBuffer(
  2,
  audioCtx.sampleRate * 3,
  audioCtx.sampleRate,
);

// Fill the buffer with white noise;
// just random values between -1.0 and 1.0
for (let channel = 0; channel < myArrayBuffer.numberOfChannels; channel++) {
  // This gives us the actual ArrayBuffer that contains the data
  const nowBuffering = myArrayBuffer.getChannelData(channel);
  for (let i = 0; i < myArrayBuffer.length; i++) {
    // Math.random() is in [0; 1.0]
    // audio needs to be in [-1.0; 1.0]
    nowBuffering[i] = Math.random() * 2 - 1;
  }
}
```

---

### Working with buffers

Using samples is a bit tricky, since we are on the web we don't have access to a _filesystem_.

How can we do it?

---

### Working with buffers

The sample needs to be accessible via the web, then we can use the Fetch API to retrieve it:

```js
fetch("https://path.to.sample.wav")
```

---

### Working with buffers

Question: Why wouldn't this work?

```js
// ! pseudo code !
const sample = fetch("https://path.to/sample.wav")

```

---

### Detour: Async Programming

It wouldn't work because:
- we have no idea how long it takes to get that sample
- our program would just have to wait

but JavaScript is single-threaded, no parallelism, what can we do?

---

### Detour: Async Programming

![[concurrent-v-parallel.png]]

---

### Detour: Async Programming

In Javascript there is something called a `Promise` which is a value that will be available (or rejected) in the future:

```js
// these doesn't seem useful, and you won't need it often, but fetch works like this
const value = new Promise((resolve) => {
  setTimeout(() => {
    resolve(true);
  }, 1000);
});
```

---

### Detour: Async Programming

We can then access the value as soon as it resolves:

```js
// our code will continue and go into the `then` block when the Promise resolves
value.then(wrappedValue => console.log(wrappedValue))
```

Or we can await the value:

```js
// the whole context (the function this runs inside) is blocked until the Promise resolves
const value = await new Promise(...)
```

---

### Working with samples

Since fetching a file takes time and we can't know how much, `fetch` returns a promise:

```js
fetch("https://path.to/sample.wav").then(response => response.arrayBuffer())
```

[`arrayBuffer()`](https://developer.mozilla.org/en-US/docs/Web/API/Response/arrayBuffer) returns another Promise containing the binary sample data.

---

### Working with samples

Let's unpack this too:

```js
fetch("...")
  .then((response) => response.arrayBuffer())
  .then((buffer) => ctx.decodeAudioData(buffer))
```

`decodeAudioData()` returns another Promise ðŸ¤¦, let's unpack this too!

---

### Working with samples

```js
fetch("...")
  .then((response) => response.arrayBuffer())
  .then((buffer) => ctx.decodeAudioData(buffer))
  .then((decodedB) => {
    const source = ctx.createBufferSource();
    source.buffer = decodedB;
    source.connect(ctx.destination);
    source.start(ctx.currentTime);
  })
```

---

Let's build something.

---

### Audio Worklets

`AudioWorklets` are the most powerful feature of the Web Audio API:

- implement custom DSP-code on the sample level
- create our own `AudioNodes`

---

### Audio Worklets

Is a custom `class` that implements a `process(inputs, outputs, parameters)` method:

```js
class PassthroughProcessor extends AudioWorkletProcessor {

  process(inputs, outputs, parameters) {
    const input = inputs[0];
    const output = outputs[0];
    for (let channel = 0; channel < input.length; ++channel) {
      const inputChannel = input[channel];
      const outputChannel = output[channel];
      for (let i = 0; i < inputChannel.length; ++i) {
        outputChannel[i] = inputChannel[i];
      }
    }
    return true;
  }
  
}

registerProcessor("passthrough-processor", PassthroughProcessor);
```

---

### Side Note

We haven't looked at `classes`. They are a way to group together data and methods. But they also serve as blueprints for how we can create certain objects (which is why e.g.  `AudioWorkletProcessor` uses it)

**Opinion**: Don't worry about classes. Classes are stupid. If you think you need a class, ask yourself if you _really_ need it. Only use them when you have to (e.g. `AudioWorkletProcessor`).

*\*Except if you like them, then use them :)*

---

### Audio Worklets

After we defined the processor, we can use it like this:

```js
// since we have to load this file via network we await for it to finish
await ctx.audioWorklet.addModule("path/to/worklet.js");

const passthrough = new AudioWorkletNode(ctx, "passthrough-processor");

passthrough.connect(masterGain);
```

---

Let's look at an example
